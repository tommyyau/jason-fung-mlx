# Project Overview: Jason Fung MLX Fine-Tuning

This project is a pipeline for fine-tuning language models on Dr. Jason Fung's medical education content using Apple's MLX framework. The goal is to create a model that mimics Dr. Fung's teaching style and communication patterns, specifically focusing on insulin resistance and fasting.

## Latest Configuration: Run 9

The current active configuration is **Run 9**, which focuses on fine-tuning the **Granite 4.0 H-Micro** model.

### Model Details
- **Base Model**: `ibm-granite/granite-4.0-h-micro`
- **Parameters**: 3 Billion (fits comfortably on 16GB Apple Silicon)
- **Architecture**: Hybrid Mixture-of-Experts (MoE) combining Mamba-2 and Transformer.
- **Active Parameters**: ~500M per inference (highly efficient).

### Training Configuration
- **Script**: `train_and_test_run9.sh`
- **Config**: `config/mlx_granite-4.0-h-micro_training.yaml`
- **Method**: LoRA (Low-Rank Adaptation)
- **Epochs**: 2 (2710 iterations)
- **Batch Size**: 1 (with gradient accumulation of 16)
- **Learning Rate**: 1e-5
- **LoRA Rank**: 16
- **LoRA Alpha**: 16
- **LoRA Dropout**: 0.1
- **Target Layers**: 16
- **Sequence Length**: 1024 tokens

### Data Format
The model is trained using a simple text format:
```json
{"text": "Question: ...\nAnswer: ..."}
```
This format is generated by `scripts/phase3-prepare-data-mlx/04c_convert_to_granite_format.py`.

## Pipeline Overview

The project uses a sophisticated multi-stage pipeline:

1.  **Phase 1: Extraction**: Downloads video transcripts from YouTube.
2.  **Phase 2: Refinement**: Uses a **two-pass approach** to generate high-quality Q&A pairs.
    -   Pass 1: Extract questions.
    -   Pass 2: Generate answers.
3.  **Phase 3: Preparation**: Converts data to MLX-compatible formats (JSONL) and splits into train/validation sets.
4.  **Phase 4: Fine-Tuning**: Trains the model using MLX LoRA.
    -   Includes fusing adapters back into the base model.
5.  **Phase 5: Conversion**: Exports the model to HuggingFace and GGUF formats for wider compatibility.

## Key Files

-   `train_and_test_run9.sh`: **Primary entry point** for the latest run. Handles training, fusing, and testing.
-   `config/mlx_granite-4.0-h-micro_training.yaml`: Detailed MLX training configuration.
-   `README.md`: Comprehensive project documentation and setup guide.
-   `scripts/`: Directory containing all pipeline scripts, organized by phase.
-   `data/mlx_training_data/`: Contains `train.jsonl` and `valid.jsonl`.

## Usage

To execute the latest training run:

```bash
./train_and_test_run9.sh
```

This script will:
1.  Train the model (approx. 80-100 mins on M1 Pro).
2.  Fuse the LoRA adapters.
3.  Run automated test prompts to verify the model's knowledge (e.g., "Should I count calories or focus on insulin?").
4.  Optionally convert the model to quantized GGUF format (~2GB).
