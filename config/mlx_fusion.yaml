# MLX Fusion Configuration
# Direct CLI usage: python -m mlx_lm fuse --model <base> --adapter-path <adapters> --save-path <output>
#
# This config is provided for reference. Fusion is simple enough that CLI arguments
# are often more convenient than a config file.

# Base model (must match the model used for training)
model: mlx-community/Llama-3.2-3B-Instruct

# Path to LoRA adapters (output from training)
adapter-path: models/jason_fung_mlx

# Output path for fused model
save-path: models/jason_fung_mlx_fused

# Dequantize model (recommended for full precision)
de-quantize: true

# Note: MLX's fuse command doesn't directly support YAML config.
# Use this as reference for CLI arguments:
#
# python -m mlx_lm fuse \
#   --model mlx-community/Llama-3.2-3B-Instruct \
#   --adapter-path models/jason_fung_mlx \
#   --save-path models/jason_fung_mlx_fused \
#   --de-quantize
