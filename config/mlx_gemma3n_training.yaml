# MLX Training Configuration
# Direct CLI usage: python -m mlx_lm lora --config config/mlx_training.yaml
#
# This config mirrors the settings in training_config.yaml but uses MLX's native format.
# For detailed parameter explanations, see docs/TRAINING_GUIDE.md

# Base model
# model: mlx-community/Llama-3.2-3B-Instruct
# model: lmstudio-community/granite-4.0-h-tiny-MLX-4bit
# model: alexgusevski/gemma-3-text-4b-it-q8-mlx
# model: lmstudio-community/gemma-3n-E4B-it-MLX-4bit - Will not work as it has already been quantized
model: google/gemma-3n-E4B

# Enable training
train: true

# Data directory (must contain train.jsonl and valid.jsonl)
data: data/mlx_training_data

# Or explicitly:
# chat_template: "<start_of_turn>user\n{user}<end_of_turn>\n<start_of_turn>model\n{assistant}<end_of_turn>"


# Fine-tuning method
fine_tune_type: lora

# Optimizer
optimizer: adamw

# LoRA configuration
num_layers: 8  # Aggressively reduced from 10 to 8 for 4B model on 16GB systems

# Training hyperparameters (optimized for 16GB RAM Apple Silicon)
learning_rate: 1e-5  # Conservative to prevent catastrophic forgetting
batch_size: 1  # Reduced to save memory
iters: 1367  # Calculated: 1 epoch × 1367 examples / 1 batch = 1367 iterations
max_seq_length: 256  # Aggressively reduced from 512 to 256 for 4B model on 16GB systems

# Gradient handling
grad_accumulation_steps: 8  # Effective batch size = 8 for stable gradients
grad_checkpoint: true  # Enable gradient checkpointing to save memory

# Dora configuration - NEW STUFF
use_dora: false  # Disable if enabled, saves memory
lora_rank: 4  # Aggressively reduced from 8 to 4 for 4B model memory constraints

# Training monitoring
# steps_per_eval: 50  # Disabled - validation causes GPU memory overflow
steps_per_report: 50  # Report progress every 50 steps
save_every: 500  # Save checkpoint every 500 steps

# Output directory for LoRA adapters
adapter_path: models/gemma-3n-E4B-it-MLX-4bit

# Reproducibility
seed: 42

# Notes:
# - Close Cursor/IDEs before training for 50-70% speedup (see docs/PERFORMANCE_OPTIMIZATION.md)
# - Training takes ~90 minutes on 16GB M1 MacBook Pro with apps closed
# - Monitor with: Activity Monitor → Memory tab (should stay GREEN)
