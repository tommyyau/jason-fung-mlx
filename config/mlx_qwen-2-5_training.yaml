# MLX Training Configuration
# Direct CLI usage: python -m mlx_lm lora --config config/mlx_training.yaml
#
# This config mirrors the settings in training_config.yaml but uses MLX's native format.
# For detailed parameter explanations, see docs/TRAINING_GUIDE.md

# Base model
# model: mlx-community/Llama-3.2-3B-Instruct
# model: lmstudio-community/granite-4.0-h-tiny-MLX-4bit
# model: alexgusevski/gemma-3-text-4b-it-q8-mlx
model: mlx-community/Qwen2.5-1.5B-Instruct-4bit

# Data directory (must contain train.jsonl and valid.jsonl)
data: data/mlx_training_data
# Output directory for LoRA adapters
adapter_path: models/Qwen2.5-1.5B-Instruct-4bit


# Enable training
train: true

# Fine-tuning method
fine_tune_type: lora

# Optimizer
optimizer: adamw

# LoRA configuration
num_layers: 12  # Reduced from 16 to preserve more base model layers

# Training hyperparameters (optimized for 16GB RAM Apple Silicon)
learning_rate: 1e-5  # Conservative to prevent catastrophic forgetting
batch_size: 1  # Reduced to save memory
iters: 1367  # Calculated: 1 epoch × 1367 examples / 1 batch = 1367 iterations
max_seq_length: 1024  # Reduced from 1024 to prevent GPU memory overflow

# Gradient handling
grad_accumulation_steps: 8  # Effective batch size = 8 for stable gradients
grad_checkpoint: true  # Enable gradient checkpointing to save memory

# Dora configuration - NEW STUFF
use_dora: false  # Disable if enabled, saves memory
lora_rank: 16  # Reduce from default 16 or 32

# Training monitoring
# steps_per_eval: 50  # Disabled - validation causes GPU memory overflow
steps_per_report: 50  # Report progress every 50 steps
save_every: 500  # Save checkpoint every 500 steps


# Reproducibility
seed: 42

# Notes:
# - Close Cursor/IDEs before training for 50-70% speedup (see docs/PERFORMANCE_OPTIMIZATION.md)
# - Training takes ~90 minutes on 16GB M1 MacBook Pro with apps closed
# - Monitor with: Activity Monitor → Memory tab (should stay GREEN)
