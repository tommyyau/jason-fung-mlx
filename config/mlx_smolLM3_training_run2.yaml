# MLX Training Configuration
# Direct CLI usage: python -m mlx_lm lora --config config/mlx_training.yaml
#
# This config mirrors the settings in training_config.yaml but uses MLX's native format.
# For detailed parameter explanations, see docs/TRAINING_GUIDE.md

# Base model
# model: mlx-community/Llama-3.2-3B-Instruct
# model: lmstudio-community/granite-4.0-h-tiny-MLX-4bit
# model: alexgusevski/gemma-3-text-4b-it-q8-mlx
# model: mlx-community/Qwen2.5-1.5B-Instruct-4bit
model: HuggingFaceTB/SmolLM3-3B

# Data directory (must contain train.jsonl and valid.jsonl)
data: data/mlx_training_data
# Output directory for LoRA adapters
adapter_path: models/SmolLM3-3B_run2


# Enable training
train: true

# Fine-tuning method
fine_tune_type: lora

# Optimizer
optimizer: adamw

# LoRA configuration
num_layers: 16  # 12 -> 16

# Training hyperparameters (optimized for 16GB RAM Apple Silicon)
learning_rate: 1e-5  # Conservative to prevent catastrophic forgetting
batch_size: 2  # Increased from 1: seq_length reduction (1024→512) frees enough memory
iters: 2051 # 3 epochs: 1367 examples / batch_size 2 = 683.5 iters/epoch × 3 = 2050.5 ≈ 2051
max_seq_length: 512  # Optimized: max entry is 511 tokens, saves ~1.5-2 GB vs 1024

# Gradient handling
grad_accumulation_steps: 4  # Effective batch size = 2×4 = 8 (same as before)
grad_checkpoint: true  # Enable gradient checkpointing to save memory

# Dora configuration - NEW STUFF
use_dora: false  # Disable if enabled, saves memory
lora_rank: 16  # Reduce from default 16 or 32

# Training monitoring
# steps_per_eval: 50  # Disabled - validation causes GPU memory overflow
steps_per_report: 50  # Report progress every 50 steps
save_every: 500  # Save checkpoint every 500 steps


# Reproducibility
seed: 42

# Notes:
# - Close Cursor/IDEs before training for 50-70% speedup (see docs/PERFORMANCE_OPTIMIZATION.md)
# - Training takes ~90 minutes on 16GB M1 MacBook Pro with apps closed
# - Monitor with: Activity Monitor → Memory tab (should stay GREEN)
