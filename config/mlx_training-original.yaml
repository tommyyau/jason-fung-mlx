# MLX Training Configuration
# Direct CLI usage: python -m mlx_lm lora --config config/mlx_training.yaml
#
# This config mirrors the settings in training_config.yaml but uses MLX's native format.
# For detailed parameter explanations, see docs/TRAINING_GUIDE.md

# Base model
model: mlx-community/Llama-3.2-3B-Instruct

# Enable training
train: true

# Data directory (must contain train.jsonl and valid.jsonl)
data: data/mlx_training_data

# Fine-tuning method
fine-tune-type: lora

# Optimizer
optimizer: adamw

# LoRA configuration
num-layers: 12  # Reduced from 16 to preserve more base model layers

# Training hyperparameters (optimized for 16GB RAM Apple Silicon)
learning-rate: 1e-5  # Conservative to prevent catastrophic forgetting
batch-size: 1  # Reduced to save memory
iters: 2734  # Calculated: 2 epochs × 1367 examples / 1 batch = 2,734 iterations
max-seq-length: 1024  # Prevents truncation of longer answers

# Gradient handling
grad-accumulation-steps: 8  # Effective batch size = 8 for stable gradients
grad-checkpoint: true  # Enable gradient checkpointing to save memory

# Training monitoring
steps-per-eval: 50  # Run validation every 50 steps
steps-per-report: 50  # Report progress every 50 steps
save-every: 500  # Save checkpoint every 500 steps

# Output directory for LoRA adapters
adapter-path: models/jason_fung_mlx

# Reproducibility
seed: 42

# Notes:
# - Close Cursor/IDEs before training for 50-70% speedup (see docs/PERFORMANCE_OPTIMIZATION.md)
# - Training takes ~90 minutes on 16GB M1 MacBook Pro with apps closed
# - Monitor with: Activity Monitor → Memory tab (should stay GREEN)
