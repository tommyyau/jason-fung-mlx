# MLX Training Configuration
# Direct CLI usage: python -m mlx_lm lora --config config/mlx_training.yaml
#
# This config mirrors the settings in training_config.yaml but uses MLX's native format.
# For detailed parameter explanations, see docs/TRAINING_GUIDE.md

# Base model
model: mlx-community/Llama-3.2-3B-Instruct

# Enable training
train: true

# Data directory (must contain train.jsonl and valid.jsonl)
data: data/mlx_training_data

# Fine-tuning method
fine_tune_type: lora

# Optimizer
optimizer: adamw

# LoRA configuration
num_layers: 12  # Reduced from 16 to preserve more base model layers

# Training hyperparameters (optimized for 16GB RAM Apple Silicon)
learning_rate: 1e-5  # Conservative to prevent catastrophic forgetting
batch_size: 1  # Reduced to save memory
iters: 4101  # Calculated: 3 epochs × 1367 examples / 1 batch = 4101 iterations
max_seq_length: 1024  # Reduced from 1024 to prevent GPU memory overflow

# Gradient handling
grad_accumulation_steps: 8  # Effective batch size = 8 for stable gradients
grad_checkpoint: true  # Enable gradient checkpointing to save memory

# Dora configuration - NEW STUFF
use_dora: false  # Disable if enabled, saves memory
lora_rank: 8  # Reduce from default 16 or 32

# Training monitoring
# steps_per_eval: 50  # Disabled - validation causes GPU memory overflow
steps_per_report: 50  # Report progress every 50 steps
save_every: 500  # Save checkpoint every 500 steps

# Output directory for LoRA adapters
adapter_path: models/jason_fung_mlx-run3

# Reproducibility
seed: 42

# Notes:
# - Close Cursor/IDEs before training for 50-70% speedup (see docs/PERFORMANCE_OPTIMIZATION.md)
# - Training takes ~90 minutes on 16GB M1 MacBook Pro with apps closed
# - Monitor with: Activity Monitor → Memory tab (should stay GREEN)
