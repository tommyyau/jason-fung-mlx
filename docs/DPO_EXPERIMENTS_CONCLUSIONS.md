# DPO Experiments: Final Conclusions

## Executive Summary

We attempted to use Direct Preference Optimization (DPO) to train Llama 3.2-3B-Instruct to prefer "insulin model" explanations over "calories in, calories out" (CICO) explanations for weight loss topics. **The experiments failed**, and we discovered fundamental limitations - not just of DPO, but of fine-tuning itself, and arguably of current transformer architecture.

**Key Finding**: The base model has a strong inherent CICO bias baked into its weights from pretraining. DPO cannot flip deeply held "beliefs" - it can only adjust style and surface patterns.

**Deeper Finding**: Fine-tuning is alchemy, not science. We adjust millions of parameters we don't understand, hoping the right weights change. Sometimes the spell works. Sometimes it doesn't. And we often can't explain why.

---

## Part 1: The Experiments

### Experiment 1: DPO with Granite-Generated Rejected Responses

**Setup:**
- Base model: Llama 3.2-3B-Instruct
- Chosen: Detailed Jason Fung-style responses (~1900 chars)
- Rejected: Generic responses generated by Granite model (~500 chars)

**Results:**
- Training diverged rapidly
- Reward diff exploded to 4.2+
- Model collapsed (produced garbage output)

**Root Cause:** Model mismatch - rejected responses were generated by Granite, not Llama. The reference log probabilities didn't match the policy model's distribution.

---

### Experiment 2: DPO with Conservative Hyperparameters

**Changes:**
- Reduced beta: 0.1 → 0.05
- Reduced learning rate: 1e-5 → 5e-6
- Increased gradient accumulation: 4 → 8

**Results:**
- Training appeared stable initially
- Loss: 0.69 → 0.18
- Reward diff: 0 → 1.6
- **Model still collapsed** - produced repetitive garbage (dots, dashes)

**Root Cause:** Even with conservative settings, the style/length gap was too large.

---

### Experiment 3: The Diagnostic Test That Changed Everything

Before attempting more DPO runs, we stepped back and tested the fundamental hypothesis: **Does Llama have an inherent bias?**

**Method:** Created 5 matched pairs of responses (same length, same style) - one CICO-biased, one insulin-biased - and measured log probabilities.

**Results:**

| Question | CICO logp | Insulin logp | Winner |
|----------|-----------|--------------|--------|
| Why do people gain weight? | -1.53 | -2.71 | CICO |
| Best way to lose weight? | -1.95 | -2.19 | CICO |
| Why do diets fail? | -2.86 | -3.47 | CICO |
| Is a calorie a calorie? | -2.56 | -2.81 | CICO |
| Why am I always hungry? | -2.16 | -2.88 | CICO |

**Summary:**
- CICO wins: **5/5 (100%)**
- Average gap: **-0.6** (significant)

**This 5-minute test revealed what hours of training couldn't:** The model fundamentally "believes" CICO. We weren't failing at DPO - we were attempting the impossible.

---

## Part 2: Why Fine-Tuning Failed

### The Fundamental Problem

```
Fine-tuning teaches: STYLE, PATTERNS, FORMAT
Fine-tuning does NOT teach: BELIEFS, KNOWLEDGE, WORLDVIEW
```

Asking DPO to make Llama prefer insulin model over CICO is asking it to change what the model "knows" - this requires pretraining-level intervention, not fine-tuning.

### Why DPO Specifically Failed

1. **Inherent Model Bias**: Llama was pretrained on internet data that heavily favors CICO explanations (the mainstream view). The model "believes" CICO at a fundamental level.

2. **DPO's Design Limitation**: DPO works by adjusting log probability ratios between chosen and rejected responses. When the model strongly prefers rejected (CICO), flipping this requires aggressive updates that destabilize the model.

3. **Length Disparity** (initial runs): Original chosen responses were 3.6x longer than rejected, creating additional signal confusion.

4. **Model Mismatch** (initial runs): Using Granite-generated rejected responses for Llama training created reference probability misalignment.

### The "Belief" Problem

LLMs encode mainstream views from their training data. Alternative viewpoints (like insulin model vs CICO) are minority perspectives in the training corpus, resulting in lower probabilities that are hard to boost without destabilizing the model.

The core question we were really asking:

> "Can I change what the model learned from terabytes of data using 300 examples?"

**The answer is: No.**

---

## Part 3: The Deeper Problem - Limits of Next-Token Prediction

### Beyond DPO: A Fundamental Architectural Issue

Current transformer-based LLMs are built on **next-token prediction**. They learn statistical patterns of what tokens follow other tokens. This is fundamentally different from understanding concepts.

When Llama "prefers" CICO over the insulin model, it's not because it *understands* metabolism - it's because CICO-style text patterns appeared more frequently in its training data. The model has learned **correlations**, not **causation** or **mechanisms**.

### What We're Really Doing With Fine-Tuning

```
Trillions of parameters → LoRA modifies 0.2% → Hope the right weights change
```

This is shockingly crude. We're:
- Adjusting millions of numbers we don't understand
- Hoping the adjustment affects the "right" internal representations
- Unable to target specific "knowledge" because we don't know where it lives

It's like performing brain surgery by slightly adjusting random neurons and hoping the patient develops a new skill.

### The Knowledge Localization Problem

Research suggests knowledge might be stored in specific "circuits" or neuron groups:
- **Anthropic's Mechanistic Interpretability** - Finding specific neurons that encode concepts
- **Knowledge Neurons** - Research showing some facts are localized
- **ROME/MEMIT** - Editing specific facts by targeting specific layers

But this work is early. We can locate simple facts ("The Eiffel Tower is in Paris") but not complex frameworks ("The carbohydrate-insulin model of obesity").

### World Models: A Different Paradigm

Emerging research on **World Models** (Yann LeCun's JEPA, Meta's V-JEPA) suggests a different approach:

| Next-Token Prediction | World Models |
|----------------------|--------------|
| Predicts text patterns | Learns environment dynamics |
| Statistical correlations | Causal relationships |
| "What usually follows X?" | "How does X affect Y?" |
| Surface patterns | Underlying mechanisms |

A world model trained on physiology might actually *understand* that insulin promotes fat storage - not just that "insulin" and "fat storage" co-occur in text.

### Could We Target "Knowledge Pockets"?

Theoretically, if we could:
1. **Identify** where "CICO beliefs" are encoded in the weights
2. **Surgically modify** just those circuits
3. **Inject** insulin model understanding in their place

...we could change the model's "beliefs" efficiently.

**Current reality:** We can't do this for complex frameworks. Mechanistic interpretability is making progress, but we're years away from "belief surgery."

### The Honest State of Affairs

| What We Can Do | What We Can't Do |
|----------------|------------------|
| Train on patterns | Instill understanding |
| Adjust surface behavior | Change deep beliefs |
| Modify style/format | Modify worldview |
| Find simple fact neurons | Find framework circuits |

---

## Part 4: How DO You Flip Fundamental Model Bias?

If preference-based fine-tuning can't change beliefs, what can?

### Methods That Can Work (With Significant Cost)

#### 1. Continued Pretraining (Most Principled)

Train on millions of tokens of insulin model content BEFORE fine-tuning.

```
Base Llama → Pretrain on Jason Fung books, papers, transcripts → SFT on Q&A
```

- **Why it works:** Shifts the underlying probability distribution at the knowledge level
- **Cost:** Significant compute, need large amounts of domain text
- **This is the "correct" answer for changing model beliefs**

#### 2. Full Fine-tuning (Not LoRA)

LoRA only modifies ~0.2% of weights. Full fine-tuning touches all parameters.

- **Why it might work:** More capacity to shift beliefs
- **Risk:** Catastrophic forgetting is significantly worse
- **Cost:** More VRAM, slower training, harder to iterate

#### 3. Knowledge Editing (ROME/MEMIT)

Surgical techniques that edit specific facts directly in the model's weights.

- **Why it might work:** Directly modifies "beliefs" about specific topics
- **Limitation:** Works for discrete facts ("Paris is capital of France"), not complex frameworks
- **Status:** Research-grade, not production-ready for worldview changes

#### 4. Train a Smaller Model From Scratch

Take a small architecture (1B params), train from scratch on curated insulin-model-heavy corpus.

- **Why it works:** No CICO bias if you control the training data
- **Cost:** Massive compute for pretraining even at small scale
- **Outcome:** Domain-specific model, loses general capabilities

#### 5. At-Scale RLHF (What OpenAI/Anthropic Do)

Thousands of preference examples, professional annotators, massive compute infrastructure.

- **Why it works:** Brute force at scale can shift distributions
- **Cost:** Millions of dollars, dedicated infrastructure, large teams
- **Not practical:** For individual or small team projects

### The Scale Problem

| What You Have | What Big Labs Have |
|---------------|-------------------|
| 300 examples | 100,000+ examples |
| 16GB MacBook | Clusters of A100s |
| Hours of compute | Weeks of compute |
| Solo experimentation | Teams of researchers |

Preference learning at scale CAN shift model behavior significantly - that's how ChatGPT became "helpful and harmless." But it requires resources beyond individual projects.

### Methods That Work at Inference Time (Practical)

| Method | Changes Model? | Effectiveness | Effort |
|--------|---------------|---------------|--------|
| RAG | No | ✅ High | Medium |
| System Prompt | No | ✅ Medium-High | Low |
| LoRA/DPO | Partially | ❌ Low for beliefs | High |

---

## Part 5: Practical Lessons Learned

### What This Project Taught Us

#### 1. Test Hypotheses Before Building

The CICO vs Insulin preference test took 5 minutes and revealed the core problem. We should have run this before attempting multiple DPO training runs.

#### 2. DPO Works Best for Subtle Preferences

DPO excels at:
- Preferring polite over impolite responses
- Choosing concise over verbose answers
- Selecting safe over harmful outputs

DPO struggles with:
- Flipping deeply held factual beliefs
- Major style overhauls (3-4x length changes)
- Domain expertise injection

#### 3. Length Normalization Isn't Enough

Even with per-token log probability normalization, large length disparities cause training instability.

#### 4. Model Collapse Signatures

Warning signs of impending collapse:
- Loss dropping below 0.3 rapidly
- Reward diff exceeding 2.0
- Training curves accelerating instead of plateauing

#### 5. Know When to Stop

Not every task is achievable with current techniques. Recognizing this early saves time and frustration.

### For Future Preference Learning Projects

1. **Test base model bias first** - Before any training, measure log probability preferences
2. **Keep preference gaps small** - Subtle style preferences, not worldview changes
3. **Match response characteristics** - Similar length, similar style, different preference
4. **Use conservative hyperparameters** - Better to undertrain than collapse
5. **Monitor for collapse early** - Stop training if reward diff exceeds 1.5-2.0

---

## Part 6: Final Reflection - Fine-Tuning is Alchemy, Not Science

### The Uncomfortable Truth

After all these experiments, here's what became clear: **Fine-tuning is sophisticated guesswork.**

| What It Looks Like | What It Actually Is |
|-------------------|---------------------|
| "Set learning rate to 1e-5" | "This number worked for someone else once" |
| "Use 8 LoRA layers" | "Somewhere between too few and too many" |
| "Beta of 0.1 for DPO" | "The paper used this, so..." |
| "Train for 3 epochs" | "Stop before it breaks, hopefully" |

### What We Don't Know

- What any specific weight represents
- Why certain hyperparameters work
- What "learning" actually happens inside
- Why it collapses when it does

### What We Do Know

- These settings worked on benchmarks
- If you change X, Y often happens
- Trial and error eventually lands somewhere
- Sometimes it just doesn't work, and we can't explain why

### The Dirty Secret of ML

Most practitioners are doing sophisticated **guess-and-check**. The theory lags far behind the practice. Papers report what worked, not the 50 failed runs before it.

When someone says "use learning rate 5e-6 for DPO" - that's not derived from first principles. It's empirical folklore passed down from experiments.

### The Black Magic of Hyperparameters

All these knobs - learning rate, beta, LoRA rank, gradient accumulation, number of layers, warmup steps, weight decay - they interact in ways nobody fully understands. We tune them based on:

- What worked in papers (different data, different models)
- Community folklore ("always start with 1e-5")
- Trial and error until something works
- Intuition developed from past failures

This isn't engineering. It's alchemy. We're mixing ingredients hoping for gold.

### When Fine-Tuning Works

Fine-tuning succeeds when:
- The task is close to what the model already does
- You have enough data (thousands, not hundreds)
- You get lucky with hyperparameters
- The shift required is small

### When Fine-Tuning Fails

Fine-tuning fails when:
- You're fighting the model's priors
- The shift required is too large
- The task requires "understanding" not "pattern matching"
- You're trying to inject knowledge, not style

### What This Experience Taught Us

This project was valuable not because it succeeded, but because it revealed the boundaries. We now know:

1. **The limits** - Not everything is fine-tunable
2. **The failure modes** - Collapse, forgetting, no learning
3. **The diagnostic approach** - Test hypotheses before building
4. **The fundamental constraints** - Beliefs vs patterns
5. **When to stop** - Some tasks are simply not achievable with current techniques

Most tutorials show the happy path. This document shows the reality.

---

## Conclusion

**DPO and preference-based fine-tuning are not suitable for changing a model's fundamental beliefs or domain expertise.** They are designed for surface-level preference adjustment within the model's existing knowledge distribution.

**This is not a failure of implementation - it's a fundamental limitation of the technique.** The experiments documented here demonstrate this limitation empirically.

For domain-specific content that contradicts mainstream views, use:
1. **System prompts** - Direct instruction (lowest effort, works)
2. **RAG** - Provide authoritative content at inference time (medium effort, works well)
3. **Continued pretraining** - Modify the model's knowledge base before fine-tuning (high effort, most principled)

### The Bigger Picture

Fine-tuning is a powerful tool, but it's not magic. It can teach style, format, and surface patterns. It cannot change what a model fundamentally "knows" or "believes." That requires either massive compute (continued pretraining at scale) or inference-time intervention (RAG/prompting).

Until we have better tools - world models that actually understand concepts, mechanistic interpretability that lets us perform "belief surgery," or architectures that go beyond next-token prediction - we work within these constraints.

The value of this project isn't the failed model. It's the understanding of why it failed, and what that teaches us about the current state of AI fine-tuning.

---

## Appendix: Technical Details

### Models Tested
- Base: `mlx-community/Llama-3.2-3B-Instruct`
- Reference (initial): `ibm-granite/granite-4.0-h-micro`

### Hyperparameters Tried

| Parameter | Run 1 | Run 2 | Run 3 (planned) |
|-----------|-------|-------|-----------------|
| beta | 0.1 | 0.05 | 0.01 |
| learning_rate | 1e-5 | 5e-6 | 1e-6 |
| grad_accumulation | 4 | 8 | 8 |
| num_layers | 14 | 14 | 8 |

### Data
- 300 DPO pairs
- Chosen: Jason Fung-style detailed responses
- Rejected: Generic model-generated responses

### Files Created During Experiments

```
config/mlx_llama-3b_dpo_run11.yaml       # DPO configuration
train_dpo_run11.sh                        # Training pipeline script
test_cico_vs_insulin_preference.py        # Bias diagnostic test
test_llama_dpo_signal.py                  # Trainability test
```

---

*Document created: 2025-11-21*
*Project: jason-fung-mlx*
