# Performance Optimization Guide

**Critical Finding**: On 16GB RAM systems, closing IDEs during training provides **50-70% speedup** by avoiding memory swapping.

---

## Critical: Memory Pressure on 16GB Systems ğŸš¨

### The Problem

MLX fine-tuning on a 16GB M1 MacBook Pro is memory-constrained:

```
MLX Training Memory Usage:
â”œâ”€ Model weights (3B params): ~6 GB
â”œâ”€ Gradients: ~6 GB
â”œâ”€ Optimizer state (Adam): ~12 GB
â”œâ”€ Activations: ~2-4 GB
â”œâ”€ MLX optimizations reduce to: ~12-14 GB
â””â”€ Total needed: 12-14 GB

Add Cursor/VSCode:
â”œâ”€ Cursor + Electron: ~3-4 GB
â”œâ”€ Language servers: ~0.5-1 GB
â”œâ”€ Extensions: ~0.5 GB
â””â”€ Total with Cursor: ~17-20 GB âŒ EXCEEDS 16GB!
```

### The Solution âœ…

**Close Cursor/IDEs during training â†’ 50-70% speedup**

```bash
# Before training:
1. Save your work in Cursor
2. Close Cursor completely (Cmd+Q)
3. Close Chrome/browsers
4. Close Slack, Discord, Spotify
5. Open native Terminal (Terminal.app or iTerm2)

# Run training:
cd /path/to/jason-fung-mlx
python3 scripts/phase4-fine-tune-model/06_train_mlx.py --lora --execute

# Results:
â”œâ”€ Memory pressure: GREEN (was RED)
â”œâ”€ Swap usage: 0 GB (was 2-3 GB)
â”œâ”€ Iteration time: 2.5s (was 5.5s)
â””â”€ Total training: 91 min (was 200+ min)
```

### Measured Performance Impact

**Validated on M1 MacBook Pro 16GB**:

| Configuration | Iteration Time | Total Training (2 epochs) | Speedup |
|---------------|----------------|---------------------------|---------|
| With Cursor open | 5-6 seconds | 180-220 minutes | Baseline |
| Cursor closed | 2.5-3 seconds | 90-110 minutes | **50-70% faster** âœ… |

**Root cause**: Memory swapping to SSD (30-60x slower than RAM)

---

## Why This Happens: Memory Swapping

### RAM vs SSD Performance

| Storage | Speed | Penalty |
|---------|-------|---------|
| **RAM** | ~200 GB/s | Baseline âœ… |
| **SSD Swap** | ~3-7 GB/s | **30-60x slower** âŒ |
| **Compressed RAM** | ~50 GB/s | 4x slower âŒ |

### What Happens When You Exceed 16GB

```
macOS Memory Pressure Response:
â”œâ”€ 0-14 GB: Everything in RAM (fast) âœ…
â”œâ”€ 14-16 GB: Start compressing memory (4x slower)
â”œâ”€ 16+ GB: Swap to SSD (30-60x slower) âŒ
â””â”€ 18+ GB: Heavy swapping + thrashing (100x+ slower) âŒâŒ
```

### Activity Monitor During Training

#### âŒ BAD: With Cursor Open (Memory Pressure)

```
Memory:
â”œâ”€ Physical Memory: 16 GB
â”œâ”€ Memory Used: 19.2 GB âŒ
â”œâ”€ Cached Files: 0 GB (purged)
â”œâ”€ Swap Used: 3.2 GB âŒ
â”œâ”€ Compressed: 2.1 GB
â””â”€ Memory Pressure: ğŸ”´ RED

What's happening:
â”œâ”€ Constantly swapping to disk
â”œâ”€ Compressing/decompressing
â”œâ”€ Purging all caches
â””â”€ Result: 5-6 seconds per iteration âŒ
```

#### âœ… GOOD: With Cursor Closed (No Pressure)

```
Memory:
â”œâ”€ Physical Memory: 16 GB
â”œâ”€ Memory Used: 14.8 GB âœ…
â”œâ”€ Cached Files: 1.2 GB
â”œâ”€ Swap Used: 0 GB âœ…
â”œâ”€ Compressed: 0.5 GB
â””â”€ Memory Pressure: ğŸŸ¢ GREEN or ğŸŸ¡ YELLOW

What's happening:
â”œâ”€ Everything fits in RAM
â”œâ”€ Minimal compression
â”œâ”€ No swapping
â””â”€ Result: 2.5 seconds per iteration âœ…
```

---

## Verification Steps

### Check Current Memory Pressure

```bash
# Method 1: Command line
sysctl vm.swapusage

# Output if good:
# vm.swapusage: total = 0.00M  used = 0.00M  free = 0.00M âœ…

# Output if swapping:
# vm.swapusage: total = 3072.00M  used = 2048.00M  free = 1024.00M âŒ

# Method 2: Activity Monitor
open -a "Activity Monitor"
# Click "Memory" tab
# Check:
#   - Swap Used: Should be 0 GB âœ…
#   - Memory Pressure: Should be green or yellow âœ…
```

### Monitor During Training

```bash
# Terminal 1: Run training
python3 scripts/phase4-fine-tune-model/06_train_mlx.py --lora --execute

# Terminal 2: Monitor memory every 2 seconds
watch -n 2 'sysctl vm.swapusage && echo && ps aux | grep python | grep -v grep | awk "{print \$3, \$4, \$11}"'

# You should see:
# - Swap stays at 0.00M âœ…
# - Python process ~95% CPU âœ…
# - Python process ~85-90% memory âœ…
```

---

## Training Performance Benchmarks

### Configuration

```yaml
Model: Llama-3.2-3B-Instruct (mlx-community)
Training: LoRA fine-tuning
Dataset: 1,095 examples (train), 342 examples (val)
Batch size: 1
Gradient accumulation: 8
Max sequence length: 1024
Epochs: 2
Hardware: M1 MacBook Pro, 16GB RAM
```

### Results

| Metric | With Cursor | Cursor Closed | Improvement |
|--------|-------------|---------------|-------------|
| **Iteration time** | 5.5s | 2.5s | 54% faster âœ… |
| **Iterations per minute** | 11 | 24 | 118% faster âœ… |
| **Time per epoch** | 100 min | 45 min | 55% faster âœ… |
| **Total training (2 epochs)** | 200 min | 90 min | 55% faster âœ… |
| **Memory pressure** | ğŸ”´ RED | ğŸŸ¢ GREEN | âœ… |
| **Swap usage** | 2-3 GB | 0 GB | âœ… |

**Time saved per training run**: ~110 minutes (1 hour 50 minutes)

---

## Best Practices for 16GB Systems

### Pre-Training Checklist

```bash
# 1. Save all work
# 2. Close these apps:
â”œâ”€ Cursor/VSCode âœ… (saves 3-4 GB)
â”œâ”€ Chrome/Safari (if many tabs) âœ… (saves 1-3 GB)
â”œâ”€ Slack/Discord âœ… (saves 0.5-1 GB)
â”œâ”€ Spotify/Music âœ… (saves 0.3-0.5 GB)
â””â”€ Docker Desktop (if running) âœ… (saves 2-4 GB)

# 3. Keep only:
â”œâ”€ Terminal (native, not IDE) âœ…
â”œâ”€ Activity Monitor (optional, for monitoring) âœ…
â””â”€ System processes âœ…

# 4. Verify memory pressure is green/yellow:
sysctl vm.swapusage  # Should show 0.00M swap used
```

### During Training

```bash
# Monitor in Activity Monitor:
â”œâ”€ Memory Pressure: ğŸŸ¢ GREEN or ğŸŸ¡ YELLOW (not ğŸ”´ RED)
â”œâ”€ Swap Used: 0 GB
â”œâ”€ Python process: ~90% memory, ~95% CPU
â””â”€ No other heavy processes running

# If memory pressure goes RED:
1. Pause training (Ctrl+C)
2. Close more applications
3. Restart training
```

### After Training

```bash
# You can reopen Cursor/apps
# Training is complete, no more memory pressure
```

---

## Alternative Optimizations (If You Must Keep Cursor Open)

If you need to keep Cursor open while training:

### Option 1: Reduce Memory Usage

```yaml
# config/training_config.yaml

# Reduce sequence length (saves 2-3 GB)
max_seq_length: 768  # Down from 1024

# Reduce LoRA layers (saves 1-2 GB)
lora:
  layers: 8  # Down from 12

# Combined savings: ~3-5 GB
# Total usage: ~9-10 GB (training) + 3-4 GB (Cursor) = ~13 GB âœ…
```

**Trade-offs**:
- Longer answers may be truncated (768 tokens)
- Less model adaptation (8 layers vs 12)
- Slightly lower final quality

### Option 2: Use `tmux` + Detach

```bash
# Start tmux session
tmux new -s training

# Run training
python3 scripts/phase4-fine-tune-model/06_train_mlx.py --lora --execute

# Detach (training continues in background): Ctrl+B, then D
# Now you can open Cursor and work on other things

# Check progress later:
tmux attach -t training
```

**Benefits**: Training runs in background, you can work in Cursor separately

### Option 3: Upgrade to 32GB RAM

```
Cost: ~$400 (if buying new Mac) or $200 (upgrade kit for some models)

With 32GB:
â”œâ”€ Can keep Cursor open âœ…
â”œâ”€ Can increase batch_size to 2-4 âœ…
â”œâ”€ Can train 7B models âœ…
â”œâ”€ Can run multiple experiments in parallel âœ…
â””â”€ Worth it if you train frequently
```

---

## Comparison: Terminal.app vs Cursor Terminal

### Cursor Terminal (Electron-based)

```
Base overhead:
â”œâ”€ Electron rendering engine: ~1 GB
â”œâ”€ VSCode extensions: ~0.5 GB
â”œâ”€ Language servers (Python, TypeScript): ~0.5-1 GB
â”œâ”€ Cursor AI features: ~1-2 GB
â”œâ”€ Terminal emulator: ~0.3 GB
â””â”€ Total: ~3-5 GB âŒ

CPU overhead:
â”œâ”€ Rendering: ~10-15% CPU
â”œâ”€ Background AI analysis: ~5-10% CPU
â”œâ”€ LSP servers: ~5% CPU
â””â”€ Total: ~20-30% CPU âŒ
```

### Terminal.app (Native macOS)

```
Overhead:
â”œâ”€ Terminal process: ~50 MB âœ…
â”œâ”€ Rendering: <5% CPU âœ…
â””â”€ Total: Negligible âœ…
```

**For long-running tasks: Native terminal is 100-200x lighter**

---

## When to Use Each Approach

### Use Native Terminal âœ…

- **Training models** (critical!)
- Long data processing (>10 minutes)
- Large file operations
- Any memory-intensive task
- When you want to close IDE but keep task running

### Use Cursor Terminal

- Quick commands (`git status`, `ls`)
- Interactive development
- Need AI assistance while working
- Short tasks (<5 minutes)
- Debugging with AI

---

## Advanced: Using `tmux` for Best of Both Worlds

### Setup

```bash
# Install tmux (if needed)
brew install tmux

# Create tmux config for better experience
cat > ~/.tmux.conf << 'EOF'
# Enable mouse support
set -g mouse on

# Increase history
set -g history-limit 10000

# Status bar
set -g status-bg blue
set -g status-fg white
EOF
```

### Workflow

```bash
# 1. Start tmux session for training
tmux new -s train

# 2. Run training
cd /path/to/jason-fung-mlx
python3 scripts/phase4-fine-tune-model/06_train_mlx.py --lora --execute

# 3. Detach (keep running): Ctrl+B, then D
# Training continues in background!

# 4. Now you can:
â”œâ”€ Open Cursor and work on other code âœ…
â”œâ”€ Close terminal completely âœ…
â”œâ”€ Restart your computer (tmux persists if you re-attach) âœ…
â””â”€ Come back hours later âœ…

# 5. Check progress anytime:
tmux attach -t train

# 6. Kill session when done:
tmux kill-session -t train
```

**Benefits**: Training isolation + freedom to work in IDE

---

## Memory Optimization Quick Reference

| Action | Memory Saved | Effort | Worth It? |
|--------|--------------|--------|-----------|
| Close Cursor | 3-4 GB | 5 seconds | âœ… YES! (50-70% speedup) |
| Close Chrome (many tabs) | 1-3 GB | 10 seconds | âœ… YES |
| Close Slack/Discord | 0.5-1 GB | 5 seconds | âœ… YES |
| Reduce max_seq_length | 2-3 GB | 1 minute | âš ï¸ Maybe (quality trade-off) |
| Reduce LoRA layers | 1-2 GB | 1 minute | âš ï¸ Maybe (quality trade-off) |
| Upgrade to 32GB | N/A | $$$ | âš ï¸ If you train frequently |

---

## Summary

### Key Findings

1. **Closing Cursor during training provides 50-70% speedup on 16GB systems** ğŸš€
2. Root cause: Memory pressure forces swapping to SSD (30-60x slower than RAM)
3. Simple fix: Close IDE, use native terminal
4. Alternative: Use `tmux` to run training in background

### One-Line Recommendation

**On 16GB systems: Always close Cursor/IDEs before training** â€” it's the single biggest performance optimization you can make (50-70% speedup for zero cost).

### Training Time Comparison

```
Original (inside Cursor, 5e-6 LR): ~200 minutes
Optimized (native terminal, 1e-5 LR): ~90 minutes

Combined speedup: 55% faster from avoiding swap + potentially faster convergence

Time saved per training run: ~110 minutes âœ…
```

---

**Last Updated**: 2025-11-09
**Validated On**: M1 MacBook Pro 16GB, macOS Sonoma
**Key Contributor**: User discovery during training runs
