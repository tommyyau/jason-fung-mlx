#!/usr/bin/env python3
"""
Step 01 – Extract Questions
───────────────────────────
Reads processed video transcripts, generates standalone user-style questions with domain tags,
and writes them to `data/generated_questions.json` for downstream answer generation.
"""

import json
import os
import sys
import asyncio
import time
from pathlib import Path
from typing import List, Dict
from openai import AsyncOpenAI
from dotenv import load_dotenv

# Load environment variables
script_dir = Path(__file__).parent
project_root = script_dir.parent.parent
possible_env_paths = [
    project_root / ".env",
    project_root / "data" / ".env",
    Path(".env"),
]

env_path = None
for path in possible_env_paths:
    if path.exists():
        env_path = path
        break

if env_path:
    load_dotenv(dotenv_path=env_path)
    print(f"✓ Loaded .env file from: {env_path.resolve()}")
else:
    load_dotenv()
    if os.getenv("OPENAI_API_KEY"):
        print("✓ Loaded .env file from default location")

# Load configuration
script_dir = Path(__file__).parent
project_root = script_dir.parent.parent
sys.path.insert(0, str(project_root / "config"))
from load_config import load_config, get_question_config

# ─────────────────────────────
# Config (loaded from training_config.yaml)
# ─────────────────────────────
config = load_config()
question_config = get_question_config()

INPUT_FILE = "data/transcripts/transcripts.jsonl"
OUTPUT_FILE = "data/generated_questions.json"
MODEL_LLM = question_config.get("model", "gpt-5-mini")
MAX_CONCURRENT = question_config.get("max_concurrent", 20)  # Number of parallel workers
DEFAULT_TEST_VIDEOS = question_config.get("default_test_videos", 5)  # Process first 5 videos by default

# Initialize OpenAI client
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("OPENAI_API_KEY not found in environment")
client = AsyncOpenAI(api_key=api_key)


# ─────────────────────────────
# Question Generation with Tags
# ─────────────────────────────
async def generate_questions_with_tags(
    transcript: str,
    video_title: str,
    video_id: str,
    client: AsyncOpenAI,
) -> List[Dict]:
    """
    Generate standalone questions with auto-generated tags from transcript.

    Returns list of dicts with: video_title, video_id, question, tags
    Tags are auto-generated by LLM within Dr. Fung's domain.
    """
    prompt = f"""You are analyzing a complete video transcript from Dr. Jason Fung. Your task is to generate important standalone questions that people would naturally ask about these topics.

Video Title: {video_title}
Video ID: {video_id}

Complete Transcript ({len(transcript):,} characters):
{transcript}

TASK:
Analyze this complete transcript and generate important standalone questions that:
1. Are self-contained (NOT follow-up questions - each question must be understandable on its own)
2. Cover important topics in Dr. Fung's domain (fasting, insulin, metabolism, diabetes, obesity, nutrition, clinical practice, physiology, etc.)
3. Can be fully answered from the content in this specific video
4. Are written as if a normal person is asking them directly - natural, conversational questions

CRITICAL - QUESTION STYLE:
- Write questions as if a normal person is asking them directly
- DO NOT reference "Dr. Fung", "this video", "according to", "in this video", etc.
- DO NOT use phrases like "What does Dr. Fung argue...", "How does Dr. Fung explain...", "What does this video say..."
- Instead, write direct questions like: "Why can't a calorie deficit exist?" NOT "Why does Dr. Fung argue that a calorie deficit cannot exist?"
- Questions should feel natural and conversational, as if someone is genuinely asking about the topic
- Examples of GOOD questions:
  * "Why can't a calorie deficit exist?"
  * "What controls whether the body releases or stores fat?"
  * "How does insulin affect weight loss?"
- Examples of BAD questions:
  * "What does Dr. Fung say about calorie deficits?" (references Dr. Fung)
  * "How does this video explain insulin?" (references the video)
  * "According to Dr. Fung, what is..." (references Dr. Fung)

For each question, also generate relevant tags. Tags should be:
- Within Dr. Fung's domain (e.g., physiology, metabolism, insulin, fasting, diabetes, obesity, nutrition, clinical_practice, weight_loss, hormones, autophagy, visceral_fat, subcutaneous_fat, time_restricted_eating, etc.)
- Specific and relevant to the question
- You determine the best tags - there's no predefined list
- Typically 2-5 tags per question

OUTPUT FORMAT:
Return ONLY a JSON array of objects. Each object must have this exact structure with fields in this order:
[
  {{
    "video_title": "{video_title}",
    "video_id": "{video_id}",
    "question": "Natural, direct question written as if a normal person is asking",
    "tags": ["tag1", "tag2", "tag3"]
  }},
  ...
]

IMPORTANT:
- Questions must be standalone (not "What about X?" or "How does that work?" - these are follow-ups)
- Questions should be specific and substantive
- Each question should be answerable from this video's content
- Write questions naturally, as if a person is asking directly - no references to Dr. Fung or the video
- Generate 5-15 questions depending on the video's content depth
- Tags should be lowercase, use underscores for multi-word tags (e.g., "time_restricted_eating")
- Return valid JSON only, no markdown code blocks
"""

    # Try up to 3 times to get valid JSON
    for attempt in range(3):
        try:
            resp = await client.chat.completions.create(
                model=MODEL_LLM,
                messages=[{"role": "user", "content": prompt}],
                max_completion_tokens=4000,
            )
            content = resp.choices[0].message.content.strip()

            # Extract JSON from markdown code blocks if present
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()

            # Parse JSON
            result = json.loads(content)

            # Ensure it's a list
            if isinstance(result, list):
                # Validate and ensure correct field order
                validated_results = []
                for item in result:
                    if isinstance(item, dict) and "question" in item:
                        validated_item = {
                            "video_title": item.get("video_title", video_title),
                            "video_id": item.get("video_id", video_id),
                            "question": item.get("question", "").strip(),
                            "tags": item.get("tags", []),
                        }
                        if validated_item["question"]:  # Only add if question is not empty
                            validated_results.append(validated_item)
                return validated_results
            elif isinstance(result, dict) and "question" in result:
                # Single question returned
                validated_item = {
                    "video_title": result.get("video_title", video_title),
                    "video_id": result.get("video_id", video_id),
                    "question": result.get("question", "").strip(),
                    "tags": result.get("tags", []),
                }
                return [validated_item] if validated_item["question"] else []
            else:
                return []

        except json.JSONDecodeError as e:
            if attempt < 2:  # Retry on JSON errors
                print(f"  ⚠️  JSON parse error (attempt {attempt + 1}/3), retrying...")
                await asyncio.sleep(2)  # Brief pause before retry
                continue
            else:
                print(f"  ⚠️  JSON parse error after 3 attempts: {str(e)[:100]}")
                print(f"     Response preview: {content[:500] if 'content' in locals() else 'N/A'}")
                return []

        except Exception as e:
            if attempt < 2:
                print(f"  ⚠️  Error (attempt {attempt + 1}/3): {type(e).__name__}, retrying...")
                await asyncio.sleep(2)
                continue
            else:
                error_msg = str(e)
                print(f"  ⚠️  Error generating questions after 3 attempts: {type(e).__name__}: {error_msg[:200]}")
                return []

    return []


# ─────────────────────────────
# Async Video Processing
# ─────────────────────────────
async def process_video_async(
    video: Dict,
    client: AsyncOpenAI,
    semaphore: asyncio.Semaphore,
    video_index: int,
    total_videos: int,
) -> List[Dict]:
    """
    Process a single video asynchronously with semaphore for concurrency control.
    """
    async with semaphore:  # Limit concurrent processing
        video_id = video["video_id"]
        title = video["title"]
        transcript = video["transcript"]

        print(f"  [{video_index}/{total_videos}] Processing: {title[:60]}...", flush=True)
        video_start = time.time()

        try:
            questions = await generate_questions_with_tags(transcript, title, video_id, client)
            video_time = time.time() - video_start

            if questions:
                print(f"     ✓ Generated {len(questions)} questions ({video_time:.1f}s)", flush=True)
            else:
                print(f"     ⚠️  No questions generated ({video_time:.1f}s)", flush=True)

            return questions

        except Exception as e:
            print(f"     ⚠️  Error processing video: {type(e).__name__}: {str(e)[:100]}", flush=True)
            return []


# ─────────────────────────────
# Main Processing
# ─────────────────────────────
async def main_async():
    """Process videos in parallel, generate questions, write to single JSON file."""
    # Read transcripts
    videos = []
    input_path = project_root / INPUT_FILE
    with open(input_path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                try:
                    data = json.loads(line)
                    if "transcript" in data and data["transcript"]:
                        videos.append(
                            {
                                "video_id": data.get("video_id", "unknown"),
                                "title": data.get("title", "Untitled"),
                                "transcript": data["transcript"],
                            }
                        )
                except json.JSONDecodeError:
                    continue

    print(f"→ Loaded {len(videos)} video transcripts")

    # Limit to first 2 videos for testing (configurable via command line)
    if len(sys.argv) > 1:
        try:
            max_videos = int(sys.argv[1])
            print(f"→ TEST MODE: Processing only first {max_videos} videos")
            videos = videos[:max_videos]
        except ValueError:
            print(f"→ Invalid number, using default: {DEFAULT_TEST_VIDEOS} videos")
            videos = videos[:DEFAULT_TEST_VIDEOS]
    else:
        print(f"→ TEST MODE: Processing first {DEFAULT_TEST_VIDEOS} videos (default)")
        videos = videos[:DEFAULT_TEST_VIDEOS]

    if not videos:
        print("❌ No videos to process")
        return

    print(f"→ Processing {len(videos)} videos with {MAX_CONCURRENT} parallel workers")
    print("→ Generating standalone questions with auto-generated tags...\n")

    start_time = time.time()

    # Create semaphore for concurrency control
    semaphore = asyncio.Semaphore(MAX_CONCURRENT)

    # Process all videos in parallel
    tasks = [
        process_video_async(video, client, semaphore, i + 1, len(videos))
        for i, video in enumerate(videos)
    ]

    results = await asyncio.gather(*tasks, return_exceptions=True)

    # Collect all questions
    all_questions = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            print(f"  ⚠️  Video {i+1} failed with exception: {type(result).__name__}")
        elif isinstance(result, list):
            all_questions.extend(result)

    total_time = time.time() - start_time

    # Write to single JSON file
    output_path = project_root / OUTPUT_FILE
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(all_questions, f, indent=2, ensure_ascii=False)

    print(f"\n✅ Done. Generated {len(all_questions)} questions from {len(videos)} videos in {total_time/60:.1f} minutes")
    print(f"   Average: {total_time/len(videos):.1f}s per video, {len(all_questions)/len(videos):.1f} questions per video")
    print(f"   Output: {output_path}")


def main():
    """Main entry point."""
    asyncio.run(main_async())


if __name__ == "__main__":
    main()


